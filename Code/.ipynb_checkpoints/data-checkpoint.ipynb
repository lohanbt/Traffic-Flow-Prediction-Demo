{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error, accuracy_score\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SCATS Number                         Location       Date  V00  V01  V02  \\\n",
      "0           970  WARRIGAL_RD N of HIGH STREET_RD  1/10/2006   86   83   52   \n",
      "1           970  WARRIGAL_RD N of HIGH STREET_RD  2/10/2006   32   28   17   \n",
      "2           970  WARRIGAL_RD N of HIGH STREET_RD  3/10/2006   26   32   21   \n",
      "3           970  WARRIGAL_RD N of HIGH STREET_RD  4/10/2006   32   22   28   \n",
      "4           970  WARRIGAL_RD N of HIGH STREET_RD  5/10/2006   40   39   21   \n",
      "\n",
      "   V03  V04  V05  V06  ...  V86  V87  V88  V89  V90  V91  V92  V93  V94  V95  \n",
      "0   58   59   44   31  ...  114   97   97   66   81   50   59   47   29   34  \n",
      "1   11    7   11    6  ...  111  102  107  114   80   60   62   48   44   26  \n",
      "2   14   10   12   13  ...  130  132  114   86   93   90   73   57   29   40  \n",
      "3   13   16    8   14  ...  115  113  132  101  113   90   78   66   52   44  \n",
      "4   11   16    9   15  ...  171  120  116  113   99   91   61   55   49   36  \n",
      "\n",
      "[5 rows x 99 columns]\n"
     ]
    }
   ],
   "source": [
    "# Only keep flow related data\n",
    "df_raw = pd.read_csv(\"data.csv\", skiprows=1, encoding='utf-8').fillna(0)\n",
    "df_raw = df_raw.drop(columns=['CD_MELWAY', 'NB_LATITUDE', 'NB_LONGITUDE', 'HF VicRoads Internal', 'VR Internal Stat',\t'VR Internal Loc', 'NB_TYPE_SURVEY', 'Unnamed: 106', 'Unnamed: 107', 'Unnamed: 108'])\n",
    "print(df_raw.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CHURCH_ST SW of BARKERS_RD' 'WARRIGAL_RD S of HIGHBURY_RD'\n",
      " 'GLENFERRIE_RD S of BURWOOD_RD']\n",
      "Location\n",
      "WARRIGAL_RD N of HIGH STREET_RD    31\n",
      "BALWYN_RD N OF BELMORE_RD          31\n",
      "SEVERN_ST S of DONCASTER_RD        31\n",
      "DONCASTER_RD E of BULLEEN_RD       31\n",
      "TOORAK_RD W OF BURKE_RD            31\n",
      "                                   ..\n",
      "BRIDGE_RD SW of BURWOOD_RD         26\n",
      "POWER_ST N of BURWOOD_RD           26\n",
      "BURWOOD_RD E of POWER_ST           26\n",
      "CANTERBURY_RD W of BALWYN_RD       26\n",
      "DENMARK_ST N of BARKERS_RD         26\n",
      "Name: count, Length: 136, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lohan\\AppData\\Local\\Temp\\ipykernel_30252\\2648910460.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop(df[df['Location'].isin(values_without_date)].index, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Make sure for 1 location, in 1 day, there are only 1 time series\n",
    "df = df_raw.drop_duplicates(subset=['Location', 'Date'], keep='first')\n",
    "\n",
    "# Drop locations without data on 31/10/2006(which is also the locations with less days recorded)\n",
    "values_with_date = df[df['Date'] == '31/10/2006']['Location'].unique()\n",
    "values_without_date = df[~df['Location'].isin(values_with_date)]['Location'].unique()\n",
    "print(values_without_date)\n",
    "df.drop(df[df['Location'].isin(values_without_date)].index, inplace=True)\n",
    "print(df['Location'].value_counts())\n",
    "\n",
    "scat_dfs = [group for _, group in df.groupby('SCATS Number')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lohan\\AppData\\Local\\Temp\\ipykernel_30252\\1630584575.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ts = scat_df.groupby('Location').apply(lambda x: x.drop(columns=['SCATS Number', 'Location', 'Date']).values.flatten().tolist())\n",
      "C:\\Users\\lohan\\AppData\\Local\\Temp\\ipykernel_30252\\1630584575.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ts = scat_df.groupby('Location').apply(lambda x: x.drop(columns=['SCATS Number', 'Location', 'Date']).values.flatten().tolist())\n",
      "C:\\Users\\lohan\\AppData\\Local\\Temp\\ipykernel_30252\\1630584575.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ts = scat_df.groupby('Location').apply(lambda x: x.drop(columns=['SCATS Number', 'Location', 'Date']).values.flatten().tolist())\n",
      "C:\\Users\\lohan\\AppData\\Local\\Temp\\ipykernel_30252\\1630584575.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ts = scat_df.groupby('Location').apply(lambda x: x.drop(columns=['SCATS Number', 'Location', 'Date']).values.flatten().tolist())\n",
      "C:\\Users\\lohan\\AppData\\Local\\Temp\\ipykernel_30252\\1630584575.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ts = scat_df.groupby('Location').apply(lambda x: x.drop(columns=['SCATS Number', 'Location', 'Date']).values.flatten().tolist())\n",
      "C:\\Users\\lohan\\AppData\\Local\\Temp\\ipykernel_30252\\1630584575.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ts = scat_df.groupby('Location').apply(lambda x: x.drop(columns=['SCATS Number', 'Location', 'Date']).values.flatten().tolist())\n",
      "C:\\Users\\lohan\\AppData\\Local\\Temp\\ipykernel_30252\\1630584575.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ts = scat_df.groupby('Location').apply(lambda x: x.drop(columns=['SCATS Number', 'Location', 'Date']).values.flatten().tolist())\n",
      "C:\\Users\\lohan\\AppData\\Local\\Temp\\ipykernel_30252\\1630584575.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ts = scat_df.groupby('Location').apply(lambda x: x.drop(columns=['SCATS Number', 'Location', 'Date']).values.flatten().tolist())\n",
      "C:\\Users\\lohan\\AppData\\Local\\Temp\\ipykernel_30252\\1630584575.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ts = scat_df.groupby('Location').apply(lambda x: x.drop(columns=['SCATS Number', 'Location', 'Date']).values.flatten().tolist())\n",
      "C:\\Users\\lohan\\AppData\\Local\\Temp\\ipykernel_30252\\1630584575.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ts = scat_df.groupby('Location').apply(lambda x: x.drop(columns=['SCATS Number', 'Location', 'Date']).values.flatten().tolist())\n",
      "C:\\Users\\lohan\\AppData\\Local\\Temp\\ipykernel_30252\\1630584575.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ts = scat_df.groupby('Location').apply(lambda x: x.drop(columns=['SCATS Number', 'Location', 'Date']).values.flatten().tolist())\n",
      "C:\\Users\\lohan\\AppData\\Local\\Temp\\ipykernel_30252\\1630584575.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ts = scat_df.groupby('Location').apply(lambda x: x.drop(columns=['SCATS Number', 'Location', 'Date']).values.flatten().tolist())\n",
      "C:\\Users\\lohan\\AppData\\Local\\Temp\\ipykernel_30252\\1630584575.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ts = scat_df.groupby('Location').apply(lambda x: x.drop(columns=['SCATS Number', 'Location', 'Date']).values.flatten().tolist())\n",
      "C:\\Users\\lohan\\AppData\\Local\\Temp\\ipykernel_30252\\1630584575.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ts = scat_df.groupby('Location').apply(lambda x: x.drop(columns=['SCATS Number', 'Location', 'Date']).values.flatten().tolist())\n",
      "C:\\Users\\lohan\\AppData\\Local\\Temp\\ipykernel_30252\\1630584575.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ts = scat_df.groupby('Location').apply(lambda x: x.drop(columns=['SCATS Number', 'Location', 'Date']).values.flatten().tolist())\n",
      "C:\\Users\\lohan\\AppData\\Local\\Temp\\ipykernel_30252\\1630584575.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ts = scat_df.groupby('Location').apply(lambda x: x.drop(columns=['SCATS Number', 'Location', 'Date']).values.flatten().tolist())\n",
      "C:\\Users\\lohan\\AppData\\Local\\Temp\\ipykernel_30252\\1630584575.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ts = scat_df.groupby('Location').apply(lambda x: x.drop(columns=['SCATS Number', 'Location', 'Date']).values.flatten().tolist())\n",
      "C:\\Users\\lohan\\AppData\\Local\\Temp\\ipykernel_30252\\1630584575.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ts = scat_df.groupby('Location').apply(lambda x: x.drop(columns=['SCATS Number', 'Location', 'Date']).values.flatten().tolist())\n",
      "C:\\Users\\lohan\\AppData\\Local\\Temp\\ipykernel_30252\\1630584575.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ts = scat_df.groupby('Location').apply(lambda x: x.drop(columns=['SCATS Number', 'Location', 'Date']).values.flatten().tolist())\n",
      "C:\\Users\\lohan\\AppData\\Local\\Temp\\ipykernel_30252\\1630584575.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ts = scat_df.groupby('Location').apply(lambda x: x.drop(columns=['SCATS Number', 'Location', 'Date']).values.flatten().tolist())\n",
      "C:\\Users\\lohan\\AppData\\Local\\Temp\\ipykernel_30252\\1630584575.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ts = scat_df.groupby('Location').apply(lambda x: x.drop(columns=['SCATS Number', 'Location', 'Date']).values.flatten().tolist())\n",
      "C:\\Users\\lohan\\AppData\\Local\\Temp\\ipykernel_30252\\1630584575.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ts = scat_df.groupby('Location').apply(lambda x: x.drop(columns=['SCATS Number', 'Location', 'Date']).values.flatten().tolist())\n",
      "C:\\Users\\lohan\\AppData\\Local\\Temp\\ipykernel_30252\\1630584575.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ts = scat_df.groupby('Location').apply(lambda x: x.drop(columns=['SCATS Number', 'Location', 'Date']).values.flatten().tolist())\n",
      "C:\\Users\\lohan\\AppData\\Local\\Temp\\ipykernel_30252\\1630584575.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ts = scat_df.groupby('Location').apply(lambda x: x.drop(columns=['SCATS Number', 'Location', 'Date']).values.flatten().tolist())\n",
      "C:\\Users\\lohan\\AppData\\Local\\Temp\\ipykernel_30252\\1630584575.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ts = scat_df.groupby('Location').apply(lambda x: x.drop(columns=['SCATS Number', 'Location', 'Date']).values.flatten().tolist())\n",
      "C:\\Users\\lohan\\AppData\\Local\\Temp\\ipykernel_30252\\1630584575.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ts = scat_df.groupby('Location').apply(lambda x: x.drop(columns=['SCATS Number', 'Location', 'Date']).values.flatten().tolist())\n",
      "C:\\Users\\lohan\\AppData\\Local\\Temp\\ipykernel_30252\\1630584575.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ts = scat_df.groupby('Location').apply(lambda x: x.drop(columns=['SCATS Number', 'Location', 'Date']).values.flatten().tolist())\n",
      "C:\\Users\\lohan\\AppData\\Local\\Temp\\ipykernel_30252\\1630584575.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ts = scat_df.groupby('Location').apply(lambda x: x.drop(columns=['SCATS Number', 'Location', 'Date']).values.flatten().tolist())\n",
      "C:\\Users\\lohan\\AppData\\Local\\Temp\\ipykernel_30252\\1630584575.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ts = scat_df.groupby('Location').apply(lambda x: x.drop(columns=['SCATS Number', 'Location', 'Date']).values.flatten().tolist())\n",
      "C:\\Users\\lohan\\AppData\\Local\\Temp\\ipykernel_30252\\1630584575.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ts = scat_df.groupby('Location').apply(lambda x: x.drop(columns=['SCATS Number', 'Location', 'Date']).values.flatten().tolist())\n",
      "C:\\Users\\lohan\\AppData\\Local\\Temp\\ipykernel_30252\\1630584575.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ts = scat_df.groupby('Location').apply(lambda x: x.drop(columns=['SCATS Number', 'Location', 'Date']).values.flatten().tolist())\n",
      "C:\\Users\\lohan\\AppData\\Local\\Temp\\ipykernel_30252\\1630584575.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ts = scat_df.groupby('Location').apply(lambda x: x.drop(columns=['SCATS Number', 'Location', 'Date']).values.flatten().tolist())\n",
      "C:\\Users\\lohan\\AppData\\Local\\Temp\\ipykernel_30252\\1630584575.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ts = scat_df.groupby('Location').apply(lambda x: x.drop(columns=['SCATS Number', 'Location', 'Date']).values.flatten().tolist())\n",
      "C:\\Users\\lohan\\AppData\\Local\\Temp\\ipykernel_30252\\1630584575.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ts = scat_df.groupby('Location').apply(lambda x: x.drop(columns=['SCATS Number', 'Location', 'Date']).values.flatten().tolist())\n",
      "C:\\Users\\lohan\\AppData\\Local\\Temp\\ipykernel_30252\\1630584575.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ts = scat_df.groupby('Location').apply(lambda x: x.drop(columns=['SCATS Number', 'Location', 'Date']).values.flatten().tolist())\n",
      "C:\\Users\\lohan\\AppData\\Local\\Temp\\ipykernel_30252\\1630584575.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ts = scat_df.groupby('Location').apply(lambda x: x.drop(columns=['SCATS Number', 'Location', 'Date']).values.flatten().tolist())\n",
      "C:\\Users\\lohan\\AppData\\Local\\Temp\\ipykernel_30252\\1630584575.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ts = scat_df.groupby('Location').apply(lambda x: x.drop(columns=['SCATS Number', 'Location', 'Date']).values.flatten().tolist())\n",
      "C:\\Users\\lohan\\AppData\\Local\\Temp\\ipykernel_30252\\1630584575.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ts = scat_df.groupby('Location').apply(lambda x: x.drop(columns=['SCATS Number', 'Location', 'Date']).values.flatten().tolist())\n",
      "C:\\Users\\lohan\\AppData\\Local\\Temp\\ipykernel_30252\\1630584575.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ts = scat_df.groupby('Location').apply(lambda x: x.drop(columns=['SCATS Number', 'Location', 'Date']).values.flatten().tolist())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{970: array([30.  , 24.75, 17.75, ..., 39.5 , 39.5 , 33.25]), 2000: array([62. , 52.5, 45. , ..., 65. , 51.5, 47. ]), 2200: array([36.5 , 32.25, 28.75, ..., 27.25, 17.  , 14.  ]), 2820: array([45. , 40. , 41. , ..., 39.5, 37.5, 29. ]), 2825: array([61., 83., 55., ..., 47., 38., 41.]), 2827: array([72.25, 68.75, 63.25, ..., 47.5 , 40.75, 35.25]), 2846: array([44.25, 30.25, 29.25, ..., 18.75, 19.5 , 14.25]), 3001: array([84.66666667, 76.66666667, 72.        , ..., 42.66666667,\n",
      "       71.        , 60.33333333]), 3002: array([58.5 , 70.25, 57.75, ..., 42.25, 40.25, 33.25]), 3120: array([81.  , 82.25, 80.  , ..., 58.5 , 58.  , 50.75]), 3122: array([45.33333333, 40.33333333, 40.        , ..., 27.33333333,\n",
      "       29.66666667, 24.33333333]), 3126: array([69.33333333, 60.66666667, 66.        , ..., 46.        ,\n",
      "       35.66666667, 36.        ]), 3127: array([45.66666667, 47.66666667, 48.        , ..., 35.        ,\n",
      "       28.        , 25.66666667]), 3180: array([26.        , 32.33333333, 26.66666667, ..., 20.33333333,\n",
      "       18.66666667, 14.        ]), 3662: array([47.5 , 55.75, 50.5 , ..., 53.  , 43.25, 43.75]), 3682: array([43.25, 35.5 , 34.25, ..., 28.  , 23.5 , 18.5 ]), 3685: array([30.5, 28.5, 18.5, ..., 42. , 36.5, 24.5]), 3804: array([30.75, 49.5 , 29.25, ..., 30.25, 24.  , 14.25]), 3812: array([43.25, 48.  , 44.5 , ..., 25.75, 19.75, 14.75]), 4030: array([47.33333333, 45.33333333, 35.33333333, ..., 41.66666667,\n",
      "       35.66666667, 31.66666667]), 4032: array([59.25, 57.  , 42.5 , ..., 38.5 , 33.  , 28.  ]), 4034: array([67.  , 58.25, 54.  , ..., 49.5 , 35.  , 26.  ]), 4035: array([65.25, 72.25, 57.75, ..., 45.  , 36.5 , 29.  ]), 4040: array([51.5       , 51.5       , 42.        , ..., 40.66666667,\n",
      "       28.5       , 26.33333333]), 4043: array([74.25, 79.75, 73.  , ..., 55.5 , 50.25, 38.5 ]), 4051: array([20.66666667, 30.        , 27.33333333, ..., 13.66666667,\n",
      "       14.33333333, 11.        ]), 4057: array([16.75, 16.25, 14.25, ..., 21.  , 15.  , 11.  ]), 4063: array([34.5 , 30.75, 34.75, ..., 28.5 , 18.5 , 16.75]), 4262: array([121.,  92.,  85., ...,  71.,  79.,  44.]), 4263: array([70.5 , 66.5 , 54.  , ..., 42.5 , 37.25, 32.75]), 4264: array([76.        , 72.66666667, 75.        , ..., 52.33333333,\n",
      "       40.66666667, 40.        ]), 4266: array([61.5 , 54.75, 48.25, ..., 41.25, 33.5 , 31.  ]), 4270: array([69.5, 65.5, 65.5, ..., 46.5, 32.5, 32. ]), 4272: array([17.        , 12.66666667, 14.        , ..., 32.66666667,\n",
      "       21.33333333, 19.66666667]), 4273: array([40.5, 47. , 30.5, ..., 34.5, 26. , 19.5]), 4321: array([31.25, 27.25, 28.25, ..., 20.  , 18.25, 18.5 ]), 4324: array([34.33333333, 29.33333333, 36.66666667, ..., 28.66666667,\n",
      "       22.66666667, 18.        ]), 4335: array([24., 29., 28., ..., 17., 19., 18.]), 4812: array([57.        , 56.66666667, 62.66666667, ..., 38.        ,\n",
      "       28.33333333, 28.        ]), 4821: array([38.75, 41.  , 39.75, ..., 30.5 , 26.5 , 24.  ])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lohan\\AppData\\Local\\Temp\\ipykernel_30252\\1630584575.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ts = scat_df.groupby('Location').apply(lambda x: x.drop(columns=['SCATS Number', 'Location', 'Date']).values.flatten().tolist())\n"
     ]
    }
   ],
   "source": [
    "# Create a python dict mapping scat site numbers to an array of traffic flow\n",
    "\n",
    "scat_to_ts = {}\n",
    "for scat_df in scat_dfs:\n",
    "  # Get time series for each side of the intersection. ts is a pandas series, indexes are location names, values are the time series of that location\n",
    "  ts = scat_df.groupby('Location').apply(lambda x: x.drop(columns=['SCATS Number', 'Location', 'Date']).values.flatten().tolist())\n",
    "\n",
    "  # Get scat number\n",
    "  scat = scat_df['SCATS Number'].values[0]\n",
    "\n",
    "  # Truncate so that everyside of the intersection has the same time series length\n",
    "  min_total_observation = min(ts.map(len).values)\n",
    "  truncated_ts = [arr[-min_total_observation:] for arr in ts.values]\n",
    "\n",
    "  # Get intersection flow by getting the average of the time series of all sides (this must be take into consideration later, since an intersection with more sides is likely to have more traffic, maybe add the values instead of averaging)\n",
    "  average_ts = np.mean(truncated_ts, axis=0)\n",
    "  scat_to_ts[scat] = average_ts\n",
    "\n",
    "print(scat_to_ts)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport csv\\nwith open(\"time_series.csv\", mode=\\'w\\', newline=\\'\\') as file:\\n  writer = csv.writer(file)\\n\\n  for key, value in scat_to_ts.items():\\n    temp = []\\n    temp.append(key)\\n    temp.extend(value)\\n    print(temp)\\n    writer.writerow(temp)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write processed time series to csv, only run ONCE\n",
    "\"\"\"\n",
    "import csv\n",
    "with open(\"time_series.csv\", mode='w', newline='') as file:\n",
    "  writer = csv.writer(file)\n",
    "\n",
    "  for key, value in scat_to_ts.items():\n",
    "    temp = []\n",
    "    temp.append(key)\n",
    "    temp.extend(value)\n",
    "    print(temp)\n",
    "    writer.writerow(temp)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'scalers/scaler_970.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m   scalers[scat] \u001b[38;5;241m=\u001b[39m scaler\n\u001b[0;32m      9\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscalers/scaler_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscat\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 10\u001b[0m     \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscalers/scaler_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mscat\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(scat_to_ts)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\COS30018env\\lib\\site-packages\\joblib\\numpy_pickle.py:552\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(value, filename, compress, protocol, cache_size)\u001b[0m\n\u001b[0;32m    550\u001b[0m         NumpyPickler(f, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_filename:\n\u001b[1;32m--> 552\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    553\u001b[0m         NumpyPickler(f, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'scalers/scaler_970.pkl'"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "# Normalize the time series & save the scalers\n",
    "scalers = {}\n",
    "for scat in scat_to_ts:\n",
    "  scaler = MinMaxScaler()\n",
    "  scat_to_ts[scat] = np.array(scat_to_ts[scat]).reshape(-1, 1)\n",
    "  scat_to_ts[scat] = scaler.fit_transform(scat_to_ts[scat]).ravel()\n",
    "  scalers[scat] = scaler\n",
    "  if not os.path.exists(f'scalers/scaler_{scat}.pkl'):\n",
    "    joblib.dump(scaler, f'scalers/scaler_{scat}.pkl')\n",
    "\n",
    "print(scat_to_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "72/72 [==============================] - 5s 13ms/step - loss: 0.0306\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0089\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0059\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0043\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0036\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0033\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0030\n",
      "Epoch 8/10\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0028\n",
      "Epoch 9/10\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0026\n",
      "Epoch 10/10\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0023\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 6s 14ms/step - loss: 0.0182\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 1s 14ms/step - loss: 0.0044\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 1s 14ms/step - loss: 0.0033\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 1s 14ms/step - loss: 0.0030\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 1s 14ms/step - loss: 0.0028\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 1s 14ms/step - loss: 0.0025\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 1s 14ms/step - loss: 0.0023\n",
      "Epoch 8/10\n",
      "72/72 [==============================] - 1s 14ms/step - loss: 0.0022\n",
      "Epoch 9/10\n",
      "72/72 [==============================] - 1s 14ms/step - loss: 0.0022\n",
      "Epoch 10/10\n",
      "72/72 [==============================] - 1s 14ms/step - loss: 0.0021\n",
      "Epoch 1/20\n",
      "72/72 [==============================] - 1s 4ms/step - loss: 0.0319 - accuracy: 0.2849\n",
      "Epoch 2/20\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0035 - accuracy: 0.4965\n",
      "Epoch 3/20\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0029 - accuracy: 0.5092\n",
      "Epoch 4/20\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0024 - accuracy: 0.5502\n",
      "Epoch 5/20\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0021 - accuracy: 0.5781\n",
      "Epoch 6/20\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0021 - accuracy: 0.5772\n",
      "Epoch 7/20\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0018 - accuracy: 0.5964\n",
      "Epoch 8/20\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0017 - accuracy: 0.5873\n",
      "Epoch 9/20\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0016 - accuracy: 0.5899\n",
      "Epoch 10/20\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0016 - accuracy: 0.5960\n",
      "Epoch 11/20\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0015 - accuracy: 0.5855\n",
      "Epoch 12/20\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0014 - accuracy: 0.6021\n",
      "Epoch 13/20\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0014 - accuracy: 0.6008\n",
      "Epoch 14/20\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0014 - accuracy: 0.5999\n",
      "Epoch 15/20\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0014 - accuracy: 0.5999\n",
      "Epoch 16/20\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.0013 - accuracy: 0.5938\n",
      "Epoch 17/20\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0013 - accuracy: 0.5868\n",
      "Epoch 18/20\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0013 - accuracy: 0.5912\n",
      "Epoch 19/20\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0012 - accuracy: 0.6008\n",
      "Epoch 20/20\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0012 - accuracy: 0.6047\n",
      "72/72 [==============================] - 0s 1ms/step\n",
      "Epoch 1/10\n",
      "144/144 [==============================] - 1s 3ms/step - loss: 0.0147 - accuracy: 0.4804\n",
      "Epoch 2/10\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 0.0025 - accuracy: 0.6211\n",
      "Epoch 3/10\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 0.0018 - accuracy: 0.6688\n",
      "Epoch 4/10\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 0.0015 - accuracy: 0.6970\n",
      "Epoch 5/10\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 0.0014 - accuracy: 0.7007\n",
      "Epoch 6/10\n",
      "144/144 [==============================] - 1s 4ms/step - loss: 0.0013 - accuracy: 0.7129\n",
      "Epoch 7/10\n",
      "144/144 [==============================] - 1s 4ms/step - loss: 0.0011 - accuracy: 0.7094\n",
      "Epoch 8/10\n",
      "144/144 [==============================] - 1s 4ms/step - loss: 0.0011 - accuracy: 0.7083\n",
      "Epoch 9/10\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 0.0011 - accuracy: 0.7116\n",
      "Epoch 10/10\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 0.0010 - accuracy: 0.7225\n",
      "144/144 [==============================] - 0s 1ms/step\n",
      "Epoch 1/5\n",
      "573/573 [==============================] - 2s 2ms/step - loss: 0.0138 - accuracy: 1.0908e-04\n",
      "Epoch 2/5\n",
      "573/573 [==============================] - 1s 2ms/step - loss: 0.0112 - accuracy: 1.0908e-04\n",
      "Epoch 3/5\n",
      "573/573 [==============================] - 1s 3ms/step - loss: 0.0111 - accuracy: 1.0908e-04\n",
      "Epoch 4/5\n",
      "573/573 [==============================] - 2s 3ms/step - loss: 0.0110 - accuracy: 1.0908e-04\n",
      "Epoch 5/5\n",
      "573/573 [==============================] - 1s 2ms/step - loss: 0.0110 - accuracy: 1.0908e-04\n",
      "18/18 [==============================] - 1s 6ms/step - loss: 0.0041\n",
      "LSTM 970 12: Loss 0.004137015901505947\n",
      "18/18 [==============================] - 1s 6ms/step - loss: 0.0018\n",
      "GRU 970 12: Loss 0.0018337630899623036\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0311 - accuracy: 0.0071\n",
      "SAEs V2 970 12: Loss [0.031107911840081215, 0.007092198356986046]\n",
      "Epoch 1/10\n",
      "67/67 [==============================] - 5s 13ms/step - loss: 34851.5977\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 34844.9336\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 1s 13ms/step - loss: 34844.9141\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 34844.8984\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 1s 13ms/step - loss: 34844.9102\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 1s 12ms/step - loss: 34844.9023\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 1s 13ms/step - loss: 34844.9141\n",
      "Epoch 8/10\n",
      "53/67 [======================>.......] - ETA: 0s - loss: 34565.3672"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 147\u001b[0m\n\u001b[0;32m    145\u001b[0m lstm \u001b[38;5;241m=\u001b[39m get_lstm((time_steps, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    146\u001b[0m lstm\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmsprop\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 147\u001b[0m \u001b[43mlstm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_trains_lstm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_trains\u001b[49m\u001b[43m[\u001b[49m\u001b[43mscat\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m lstm\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/lstm/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscat\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# === Train GRU ===\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\COS30018env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\COS30018env\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\COS30018env\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\COS30018env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\COS30018env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\COS30018env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\COS30018env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\COS30018env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\COS30018env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from model import get_lstm, get_gru\n",
    "\n",
    "def split_data(flow, lags):\n",
    "  # Split data for 1 scat\n",
    "  # flow: array of flow every 15 mins\n",
    "  split_index = int(len(flow) * 0.8)  # 80% for training, 20% for testing\n",
    "\n",
    "  train_data = np.array(flow[:split_index])\n",
    "  test_data = np.array(flow[split_index:])\n",
    "  \n",
    "  train, test = [], []\n",
    "\n",
    "  for i in range(lags, len(train_data)):\n",
    "    train.append(train_data[i - lags: i + 1])\n",
    "  for i in range(lags, len(test_data)):\n",
    "    test.append(test_data[i - lags: i + 1])\n",
    "\n",
    "  train = np.array(train)\n",
    "  test = np.array(test)\n",
    "  np.random.shuffle(train)\n",
    "\n",
    "  X_train = train[:, :-1]\n",
    "  y_train = train[:, -1]\n",
    "  X_test = test[:, :-1]\n",
    "  y_test = test[:, -1]\n",
    "\n",
    "  return X_train, y_train, X_test, y_test\n",
    "\n",
    "result = {}\n",
    "\n",
    "X_trains, y_trains, X_tests, y_tests = {}, {}, {}, {}\n",
    "\n",
    "\n",
    "############################## TRAIN MODELS & TEST ##############################\n",
    "#### split_data(flow, x) change x for different input length\n",
    "from model import get_lstm, get_gru, get_saes_v2\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "\n",
    "def train_seas_v2(input_dim, X_train, y_train, name, config):\n",
    "    \"\"\"Train the SAEs V2 model.\"\"\"\n",
    "    hidden_layers = [512, 256, 128]\n",
    "    batch_size = config[\"batch\"]\n",
    "\n",
    "    # First SAE model\n",
    "    autoencoder_1 = get_saes_v2(input_dim, hidden_layers, input_dim)\n",
    "    autoencoder_1.compile(metrics=['accuracy'], loss='mean_squared_error', optimizer='adam')\n",
    "    autoencoder_1.fit(X_train, X_train, epochs=20, batch_size=batch_size)\n",
    "\n",
    "    # Second SAE model\n",
    "    autoencoder_2_input = autoencoder_1.predict(X_train)\n",
    "    autoencoder_2_input = np.concatenate((autoencoder_2_input, X_train))\n",
    "    autoencoder_2 = get_saes_v2(input_dim, hidden_layers, input_dim)\n",
    "    autoencoder_2.compile(metrics=['accuracy'], loss='mean_squared_error', optimizer='adam')\n",
    "    autoencoder_2.fit(autoencoder_2_input, autoencoder_2_input, epochs=10, batch_size=batch_size)\n",
    "\n",
    "    # Third SAE model\n",
    "    autoencoder_3_input = autoencoder_2.predict(autoencoder_2_input)\n",
    "    autoencoder_3_input = np.concatenate((autoencoder_3_input, autoencoder_2_input))\n",
    "    autoencoder_3 = get_saes_v2(input_dim, hidden_layers, input_dim, finalDense=True)\n",
    "    autoencoder_3.compile(metrics=['accuracy'], loss='mean_squared_error', optimizer='adam')\n",
    "    autoencoder_3.fit(autoencoder_3_input, autoencoder_3_input, epochs=5, batch_size=16)\n",
    "\n",
    "    # Save final model\n",
    "    autoencoder_3.save(model_path)\n",
    "    #df = pd.DataFrame.from_dict(autoencoder_3.history.history)\n",
    "    #df.to_csv(f'model/{name}_loss.csv', encoding='utf-8', index=False)\n",
    "    return autoencoder_3\n",
    "\"\"\"\n",
    "for scat in scat_to_ts:\n",
    "    \n",
    "  # Split data for each scat\n",
    "    flow = scat_to_ts[scat]\n",
    "    X_trains[scat], y_trains[scat], X_tests[scat], y_tests[scat] = split_data(flow, 12)\n",
    "\n",
    "    # Reshape inputs to have shape (samples, time_steps, features)\n",
    "    X_trains[scat] = X_trains[scat].reshape((X_trains[scat].shape[0], X_trains[scat].shape[1], 1))\n",
    "    X_tests[scat] = X_tests[scat].reshape((X_tests[scat].shape[0], X_tests[scat].shape[1], 1))\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    time_steps = X_trains[scat].shape[1]  # This should be 12 or 96, not len(flow) \n",
    "\n",
    "\n",
    "    \n",
    "    # Get LSTM\n",
    "    lstm = get_lstm((time_steps, 1))\n",
    "\n",
    "    \n",
    "\n",
    "    lstm.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
    "    lstm.fit(X_trains[scat], y_trains[scat], epochs=10, batch_size=32)\n",
    "    lstm.save(f\"models/v2/lstm/{scat}.h5\")\n",
    "  \n",
    "    y_pred = lstm.predict(X_tests[scat])\n",
    "    result[scat] = lstm.evaluate(X_tests[scat], y_tests[scat])\n",
    "    \n",
    "\n",
    "    # Train and save GRU\n",
    "    gru = get_gru((time_steps, 1))\n",
    "\n",
    "    gru.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
    "    gru.fit(X_trains[scat], y_trains[scat], epochs=10, batch_size=32)\n",
    "    \n",
    "    # Save the GRU model\n",
    "    gru.save(f\"models/v2/gru/{scat}.h5\")\n",
    "    \n",
    "    # Evaluate the GRU model\n",
    "    y_pred_gru = gru.predict(X_tests[scat])\n",
    "    result_gru = gru.evaluate(X_tests[scat], y_tests[scat])\n",
    "\n",
    "    #saesv2\n",
    "\n",
    "    #Reshape for saes (2D)\n",
    "    X_trains[scat] = X_trains[scat].reshape((X_trains[scat].shape[0], X_trains[scat].shape[1]))\n",
    "    X_tests[scat] = X_tests[scat].reshape((X_tests[scat].shape[0], X_tests[scat].shape[1]))\n",
    "    input_dim = X_trains[scat].shape[1]  # Get the number of features\n",
    "    \n",
    "    \n",
    "    saesv2 = train_seas_v2(input_dim, X_trains[scat], y_trains[scat], scat, {\"batch\": 32, \"epochs\": 2})\n",
    "    \n",
    "    \n",
    "    y_pred_saes_v2 = saesv2.predict(X_tests[scat])\n",
    "    result_saes_v2 = saesv2.evaluate(X_tests[scat], y_tests[scat])\n",
    "    \"\"\"\n",
    "    \n",
    "for lags, folder in [(12, 'v1'), (96, 'v2')]:\n",
    "    for scat in scat_to_ts:\n",
    "        # Split data for each scat\n",
    "        flow = scat_to_ts[scat]\n",
    "        X_trains[scat], y_trains[scat], X_tests[scat], y_tests[scat] = split_data(flow, lags)\n",
    "\n",
    "        # Reshape inputs for LSTM/GRU (3D: samples, time_steps, features)\n",
    "        X_trains_lstm = X_trains[scat].reshape((X_trains[scat].shape[0], X_trains[scat].shape[1], 1))\n",
    "        X_tests_lstm = X_tests[scat].reshape((X_tests[scat].shape[0], X_tests[scat].shape[1], 1))\n",
    "\n",
    "        # Get the time_steps (lags) and features\n",
    "        time_steps = X_trains_lstm.shape[1]\n",
    "\n",
    "        # === Train LSTM ===\n",
    "        lstm = get_lstm((time_steps, 1))\n",
    "        lstm.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
    "        lstm.fit(X_trains_lstm, y_trains[scat], epochs=10, batch_size=32)\n",
    "        lstm.save(f\"models/{folder}/lstm/{scat}.h5\")\n",
    "\n",
    "        # === Train GRU ===\n",
    "        gru = get_gru((time_steps, 1))\n",
    "        gru.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
    "        gru.fit(X_trains_lstm, y_trains[scat], epochs=10, batch_size=32)\n",
    "        gru.save(f\"models/{folder}/gru/{scat}.h5\")\n",
    "\n",
    "        # === Train SAEs V2 ===\n",
    "        # Reshape inputs for SAEs V2 (2D: samples, features)\n",
    "        X_trains_saes = X_trains[scat].reshape((X_trains[scat].shape[0], X_trains[scat].shape[1]))\n",
    "        X_tests_saes = X_tests[scat].reshape((X_tests[scat].shape[0], X_tests[scat].shape[1]))\n",
    "        input_dim = X_trains_saes.shape[1]\n",
    "\n",
    "        model_path = f\"models/{folder}/saesv2/{scat}.h5\"\n",
    "        saesv2 = train_seas_v2(input_dim, X_trains_saes, y_trains[scat], model_path, {\"batch\": 32, \"epochs\": 10})\n",
    "\n",
    "        # Evaluate models\n",
    "        # Evaluate LSTM\n",
    "        result_lstm = lstm.evaluate(X_tests_lstm, y_tests[scat])\n",
    "        print(f\"LSTM {scat} {lags}: Loss {result_lstm}\")\n",
    "\n",
    "        # Evaluate GRU\n",
    "        result_gru = gru.evaluate(X_tests_lstm, y_tests[scat])\n",
    "        print(f\"GRU {scat} {lags}: Loss {result_gru}\")\n",
    "\n",
    "        # Evaluate SAEs V2\n",
    "        result_saes_v2 = saesv2.evaluate(X_tests_saes, y_tests[scat])\n",
    "        print(f\"SAEs V2 {scat} {lags}: Loss {result_saes_v2}\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "############################## LOAD LSTM MODELS & TEST ##############################\n",
    "\"\"\"scaled_result = {}\n",
    "scaled_true = {}\n",
    "\n",
    "for scat in scat_to_ts:\n",
    "  flow = scat_to_ts[scat]\n",
    "  X_trains[scat], y_trains[scat], X_tests[scat], y_tests[scat] = split_data(flow, 12)\n",
    "  model = load_model(f\"models/v1/lstm/{scat}.h5\")\n",
    "  \n",
    "  result[scat] = model.predict(X_tests[scat])\n",
    "  scaled_result[scat] = scalers[scat].inverse_transform(result[scat].reshape(-1, 1)).ravel()\n",
    "  scaled_true[scat] = scalers[scat].inverse_transform(y_tests[scat].reshape(-1, 1)).ravel()\n",
    "\n",
    "for scat in result:\n",
    "  for i in range(len(scaled_result[scat])):\n",
    "    print(scaled_result[scat][i])\n",
    "    print(scaled_true[scat][i])\n",
    "    print(\"\\n\")\n",
    "  break\n",
    "  \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_absolute_error\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m))\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "plt.figure(figsize=(10, 20))\n",
    "\n",
    "  # Plot actual values\n",
    "plt.plot(scaled_true[970], label='Actual', marker='o')\n",
    "\n",
    "# Plot predicted values\n",
    "plt.plot(scaled_result[970], label='Predicted', marker='x')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Values')\n",
    "plt.title('Actual vs Predicted Values for SCAT ' + str(scat))\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "print(mean_absolute_error(scaled_true[970], scaled_result[970]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### PREDICTION ############################\n",
    "import os \n",
    "from tensorflow.keras.models import load_model\n",
    "from main import plot_results\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "\n",
    "path = \"models/lstm/\"\n",
    "\n",
    "model_files = [f for f in os.listdir(path) if f.endswith('.h5')]\n",
    "\n",
    "\n",
    "for model_file in model_files:\n",
    "  #scat = int(model_file.split('.')[0])\n",
    "  scat = 970\n",
    "  #model = load_model(path + model_file)\n",
    "  model = load_model(path + \"970.h5\")\n",
    "  prediction = model.predict(X_tests[scat])\n",
    "  y_pred = np.reshape(prediction, (prediction.shape[0],))\n",
    "\n",
    "  plt.figure(figsize=(10, 20))\n",
    "\n",
    "  # Plot actual values\n",
    "  plt.plot(y_tests[scat], label='Actual', marker='o')\n",
    "\n",
    "  # Plot predicted values\n",
    "  plt.plot(y_pred, label='Predicted', marker='x')\n",
    "\n",
    "  # Add labels and title\n",
    "  plt.xlabel('Index')\n",
    "  plt.ylabel('Values')\n",
    "  plt.title('Actual vs Predicted Values for SCAT ' + str(scat))\n",
    "\n",
    "  # Add a legend\n",
    "  plt.legend()\n",
    "\n",
    "  # Show the plot\n",
    "  plt.show()\n",
    "  print(mean_absolute_error(y_tests[970], y_pred))\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
